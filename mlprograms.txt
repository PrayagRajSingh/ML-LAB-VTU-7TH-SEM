#1st program
import csv
a=[]
with open('enjoysport.csv','r')as csvfile:
    for row in csv.reader(csvfile):
        a.append(row)
    print(a)
    
print('\n The no of training instances are',len(a)-1)
num_attr=len(a[0])-1
print(num_attr)
print('The inital hypothesis is')
hypothesis=[0]*num_attr
print(hypothesis)

for i in range(0,len(a)):
    if a[i][num_attr]=='yes':
        for j in range(0,num_attr):
            if hypothesis[j]==0 or hypothesis[j]==a[i][j]:
                hypothesis[j]=a[i][j]
            else:
                hypothesis[j]='?'
    print('the hypothesis of {} is:\n'.format(i+1),hypothesis)
print('The maximally specific hypothesis is')
print(hypothesis)

output:
    [['sky', 'airtemp', 'humidity', 'wind', 'water', 'forcast', 'enjoysport'], ['sunny', 'warm', 'normal', 'strong', 'warm', 'same', 'yes'], ['sunny', 'warm', 'high', 'strong', 'warm', 'same', 'yes'], ['rainy', 'cold', 'high', 'strong', 'warm', 'change', 'no'], ['sunny', 'warm', 'high', 'strong', 'cool', 'change', 'yes']]

 The no of training instances are 4
6
The inital hypothesis is
[0, 0, 0, 0, 0, 0]
the hypothesis of 1 is:
 [0, 0, 0, 0, 0, 0]
the hypothesis of 2 is:
 ['sunny', 'warm', 'normal', 'strong', 'warm', 'same']
the hypothesis of 3 is:
 ['sunny', 'warm', '?', 'strong', 'warm', 'same']
the hypothesis of 4 is:
 ['sunny', 'warm', '?', 'strong', 'warm', 'same']
the hypothesis of 5 is:
 ['sunny', 'warm', '?', 'strong', '?', '?']
The maximally specific hypothesis is
['sunny', 'warm', '?', 'strong', '?', '?']
-------------------------------------------------------------------------------------------------------------------------------------------------------
#2nd program
import pandas as pd
import numpy as np
import csv
data=pd.DataFrame(data=pd.read_csv('enjoysport.csv'))
concepts=np.array(data.iloc[:,0:-1])
print(concepts)
target=np.array(data.iloc[:,-1])
print(target)

def learn(concepts,target):
    specific_h=concepts[0].copy()
    print('initial specific and general_h')
    print('specific_h is: ',specific_h)
    general_h=[["?" for i in range(len(specific_h))] for i in range(len(specific_h))]
    print('general_h is ',general_h)
    for i,h in enumerate(concepts):
        if target[i]=='yes':
            for x in range(len(specific_h)):
                if h[x]!=specific_h[x]:
                    specific_h[x]='?'
                    general_h[x][x]='?'
        if target[i]=='no':
            for x in range(len(specific_h)):
                if h[x]!=specific_h[x]:
                    general_h[x][x]=specific_h[x]
                else:
                    general_h[x][x]='?'
        print('steps in candidate elimination algorithm',i+1)
        print(specific_h)
        print(general_h)
    indices=[i for i,val in enumerate(general_h)if val==['?','?','?','?','?','?']]
    for i in indices:
        general_h.remove(['?','?','?','?','?','?'])
    return specific_h,general_h
s_final,g_final=learn(concepts,target)
print('final specific_h is ',s_final,sep="\n")
print('final general_h is ',g_final,sep="\n")   

output:
[['sunny' 'warm' 'normal' 'strong' 'warm' 'same']
 ['sunny' 'warm' 'high' 'strong' 'warm' 'same']
 ['rainy' 'cold' 'high' 'strong' 'warm' 'change']
 ['sunny' 'warm' 'high' 'strong' 'cool' 'change']]
['yes' 'yes' 'no' 'yes']
initial specific and general_h
specific_h is:  ['sunny' 'warm' 'normal' 'strong' 'warm' 'same']
general_h is  [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]
steps in candidate elimination algorithm 1
['sunny' 'warm' 'normal' 'strong' 'warm' 'same']
[['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]
steps in candidate elimination algorithm 2
['sunny' 'warm' '?' 'strong' 'warm' 'same']
[['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]
steps in candidate elimination algorithm 3
['sunny' 'warm' '?' 'strong' 'warm' 'same']
[['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', 'same']]
steps in candidate elimination algorithm 4
['sunny' 'warm' '?' 'strong' '?' '?']
[['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]
final specific_h is 
['sunny' 'warm' '?' 'strong' '?' '?']
final general_h is 
[['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?']]
â€‹-----------------------------------------------------------------------------------------------
 #3rd program
import math
import csv

def load_csv(filename):
    lines = csv.reader(open(filename, "r"));
    dataset = list(lines)
    headers = dataset.pop(0)
    return dataset, headers

class Node:
    def __init__(self, attribute):
        self.attribute = attribute
        self.children = []
        self.answer = "" #null indicates children exists 
        
def subtables(data, col, delete): # after getting outlook overcast sunny rain seperation
    dic = {}
    coldata = [ row[col] for row in data]
    attr = list(set(coldata))
    
    for k in attr:
        dic[k] = []
        
    for y in range(len(data)):
        key = data[y][col]
        if delete:
            del data[y][col]
        dic[key].append(data[y])
        
    return attr, dic    

def entropy(S):
    attr = list(set(S))
    if len(attr) == 1: #if all are +ve/-ve then entropy = 0
        return 0
    
    counts = [0,0] # Only two values possible 'yes' or 'no'
    for i in range(2):
        counts[i] = sum( [1 for x in S if attr[i] == x] ) / (len(S) * 1.0)
    
    sums = 0
    for cnt in counts:
        sums += -1 * cnt * math.log(cnt, 2)
    return sums

def compute_gain(data, col):
    attValues, dic = subtables(data, col, delete=False)
    
    total_entropy = entropy([row[-1] for row in data])
    
  
    for x in range(len(attValues)):
        ratio = len(dic[attValues[x]]) / ( len(data) * 1.0)
        entro = entropy([row[-1] for row in dic[attValues[x]]])
        total_entropy -= ratio*entro
    return total_entropy

def build_tree(data, features):
    lastcol = [row[-1] for row in data] #last value in each row
    if (len(set(lastcol))) == 1: # If all samples have same labels return that label
        node=Node("")
        node.answer = lastcol[0]
        return node
    n = len(data[0])-1 #except the last column
    gains = [compute_gain(data, col) for col in range(n) ]# to compute gain
    split = gains.index(max(gains)) # Find max gains and returns index
    node = Node(features[split]) # 'node' stores attribute selected
    #del (features[split])
    fea = features[:split]+features[split+1:]

    
    attr, dic = subtables(data, split, delete=True)# Data will be spilt in subtables
    for x in range(len(attr)):
        child = build_tree(dic[attr[x]], fea)
        node.children.append((attr[x], child))
    return node

def print_tree(node, level):
    if node.answer != "":
        print(" "*level, node.answer) # Displays leaf node yes/no
        return
    print(" "*level, node.attribute) # Displays attribute Name
    for value, n in node.children:
        print(" "*(level+1), value)
        print_tree(n, level + 2)
        
def classify(node,x_test,features):
    if node.answer != "":
        print(node.answer)
        return

    pos = features.index(node.attribute)
    for value, n in node.children:
        if x_test[pos]==value:
            classify(n,x_test,features)
        
dataset, features = load_csv("id3.csv") # Read Tennis data
node = build_tree(dataset, features)
# print(dataset)
# print('features',features)

print("The decision tree for the dataset using ID3 algorithm is ")
print_tree(node, 0)

testdata, features = load_csv("id3_test_1 (1).csv")
for xtest in testdata:
    print("The test instance : ",xtest)
    print("The predicted label : ", end="")
    classify(node,xtest,features)

output:

The decision tree for the dataset using ID3 algorithm is 
 Outlook
  rain
   Wind
    weak
     yes
    strong
     no
  overcast
   yes
  sunny
   Humidity
    high
     no
    normal
     yes
The test instance :  ['rain', 'cool', 'normal', 'strong']
The predicted label : no
The test instance :  ['sunny', 'mild', 'normal', 'strong']
The predicted label : yes
-------------------------------------------------------------------------------------------

#4th program
import numpy as np
import pandas as pd
X=np.array(([2,9],[1,5],[3,6]),dtype=float)
y=np.array(([92],[86],[89]),dtype=float)
X = X/np.amax(X,axis=0) 
y = y/100
def sigmoid(x):
    return 1/(1+np.exp(-x))
def derivative_sigmoid(x):
    return x*(1-x)
epoch=1000
lr=0.2
input_neurons=2
hidden_neurons=3
output_neurons=1
wh=np.random.uniform(size=(input_neurons,hidden_neurons))
bh=np.random.uniform(size=(1,hidden_neurons))
wout=np.random.uniform(size=(hidden_neurons,output_neurons))
bout=np.random.uniform(size=(1,output_neurons))
for i in range(epoch):
    hinp1=np.dot(X,wh)
    hinp=hinp1+bh
    hidden_act=sigmoid(hinp)
    outinp1=np.dot(hidden_act,wout)
    outinp=outinp1+bout
    output=sigmoid(outinp)
    EO=y-output
    Egrad=derivative_sigmoid(output)
    d_output=EO*Egrad
    Ehidden=d_output.dot(wout.T)
    hiddengrad=derivative_sigmoid(hidden_act)
    d_hidden=Ehidden*hiddengrad
    wout+=hidden_act.T.dot(d_output)*lr
    wh+=X.T.dot(d_hidden)*lr
    
print(' Input\n'+str(X))
print(' Actual output\n'+str(y))
print('Output\n',output)
    
output:

Input
[[0.66666667 1.        ]
 [0.33333333 0.55555556]
 [1.         0.66666667]]
 Actual output
[[0.92]
 [0.86]
 [0.89]]
Output
 [[0.89604437]
 [0.87933866]
 [0.89460831]]
-----------------------------------------------------------------------------------------------

 #5th program
import csv,random,math
import statistics as st
def loadCsv(filename):
    lines=csv.reader(open(filename,"r"))
    dataset=list(lines)
    for i in range(len(dataset)):
        dataset[i]=[float(x) for x in dataset[i]]
    return dataset
def splitdataset(dataset,splitratio):
    testsize=int(len(dataset)*splitratio)
    trainset=list(dataset)
    testset=[]
    while len(testset)<testsize:
        index=random.randrange(len(trainset))
        testset.append(trainset.pop(index))
    return[trainset,testset]
def separatebyclass(dataset):
    separated={}
    for i in range(len(dataset)):
        x=dataset[i]
        if(x[-1] not in separated):
            separated[x[-1]]=[]
        separated[x[-1]].append(x)
    return separated
def compute_mean_std(dataset):
    mean_std=[(st.mean(attribute),st.stdev(attribute))
             for attribute in zip(*dataset)];
    del mean_std[-1]
    return mean_std
def summarizebyclass(dataset):
    separated=separatebyclass(dataset)
    summary={}
    for classvalue,instances in separated.items():
        summary[classvalue]=compute_mean_std(instances)
    return summary
def estimate_probability(x,mean,stdev):
    exponent=math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))
    return (1/(math.sqrt(2*math.pi)*stdev))*exponent
def calculateclassprobabilities(summaries,testVector):
    p={}
    for classvalue,classsummaries in summaries.items():
        p[classvalue]=1
        for i in range(len(classsummaries)):
            mean,stdev=classsummaries[i]
            x=testVector[i]
            p[classvalue]*=estimate_probability(x,mean,stdev)
    return p
def predict(summaries,testVector):
    all_p=calculateclassprobabilities(summaries,testVector)
    bestLabel,bestProb=None,-1
    for lbl,p in all_p.items():
        if bestLabel is None or p > bestProb:
            bestProb=p
            bestLabel=lbl
    return bestLabel
def perform_classification(summaries,testset):
    predictions=[]
    for i in range(len(testset)):
        result=predict(summaries,testset[i])
        predictions.append(result)
    return predictions

def get_accuracy(testset,predictions):
    correct=0
    for i in range(len(testset)):
        if testset[i][-1]==predictions[i]:
            correct+=1
    return (correct/float(len(testset)))*100.0
        
dataset = loadCsv('data5.csv');
print('Total instances avaialable',len(dataset))
print('total attrbutes present',len(dataset[0])-1)
print('frst 5 instances')
for i in range(5):
    print(i+1,":",dataset[i])
splitratio=0.2
trainingset,testset=splitdataset(dataset,splitratio)
print('total trainset is {0}\n test set is {1}\n'.format(len(trainingset),len(testset)))
summaries=summarizebyclass(trainingset)
predictions=perform_classification(summaries,testset)
accuracy=get_accuracy(testset,predictions)
print('accuracy is:',accuracy)

output:

Total instances avaialable 768
total attrbutes present 8
frst 5 instances
1 : [6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]
2 : [1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0, 0.0]
3 : [8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0, 1.0]
4 : [1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0, 0.0]
5 : [0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0, 1.0]
total trainset is 615
 test set is 153

accuracy is: 72.54901960784314
------------------------------------------------------------------------------------------------
#6th program
import pandas as pd
msg=pd.read_csv('naivetext.csv',names=['message','label'])
print('Total instances in the dataset',msg.shape[0])

msg['labelnum']=msg.label.map({'pos':1,'neg':0})
X=msg.message
Y=msg.labelnum

print('\nThe message and label for first 5 instances are listed below')
X5,Y5=X[0:5],msg.label[0:5]
for x,y in zip(X5,Y5):
    print(x,',',y)
    
from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(X,Y)
print('\nDataset is split into Training and Testing samples')
print('Total training instances :', xtrain.shape[0])
print('Total testing instances :', xtest.shape[0])


from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
xtrain_dtm = count_vect.fit_transform(xtrain) 
xtest_dtm = count_vect.transform(xtest)
print('\nTotal features extracted using CountVectorizer:',xtrain_dtm.shape[1])


print('\nFeatures for first 5 training instances are listed below')
df=pd.DataFrame(xtrain_dtm.toarray(),columns=count_vect.get_feature_names())
print(df[0:5])

from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB().fit(xtrain_dtm,ytrain)
predicted = clf.predict(xtest_dtm)


print('\nClassstification results of testing samples are given below')
for doc, p in zip(xtest, predicted):
    pred = 'pos' if p==1 else 'neg'
    print('%s -> %s ' % (doc, pred))

from sklearn import metrics
print('\nAccuracy metrics')
print('Accuracy of the classifer is',metrics.accuracy_score(ytest,predicted))
print('Recall :',metrics.recall_score(ytest,predicted),
'\nPrecison :',metrics.precision_score(ytest,predicted))
print('Confusion matrix')
print(metrics.confusion_matrix(ytest,predicted))

output:

Total instances in the dataset: 18

The message and its label of first 5 instances are listed below
I love this sandwich , pos
This is an amazing place , pos
I feel very good about these beers , pos
This is my best work , pos
What an awesome view , pos

Dataset is split into Training and Testing samples
Total training instances : 13
Total testing instances : 5

Total features extracted using CountVectorizer: 45

Features for first 5 training instances are listed below
   about  am  an  awesome  bad  beers  best  can  deal  do  ...  this  tired  \
0      0   0   1        1    0      0     0    0     0   0  ...     1      0   
1      0   0   0        0    0      0     0    0     0   0  ...     0      0   
2      1   0   0        0    0      1     0    0     0   0  ...     0      0   
3      0   0   0        0    0      0     0    0     0   1  ...     1      0   
4      0   0   0        0    0      0     0    1     1   0  ...     1      0   

   to  today  very  view  went  what  with  work  
0   0      0     0     0     0     0     0     0  
1   0      0     0     0     0     0     0     0  
2   0      0     1     0     0     0     0     0  
3   0      0     0     0     0     0     0     0  
4   0      0     0     0     0     0     1     0  

[5 rows x 45 columns]

Classstification results of testing samples are given below
My boss is horrible -> neg 
This is an amazing place -> pos 
We will have good fun tomorrow -> pos 
I am sick and tired of this place -> neg 
I love to dance -> neg 

Accuracy metrics
Accuracy of the classifer is 0.8
Recall : 0.6666666666666666 
Precison : 1.0
Confusion matrix
[[2 0]
 [1 2]]
---------------------------------------------------------------------------------------------
#8th program
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import pandas as pd
import numpy as np
from sklearn import datasets
iris=datasets.load_iris()
X=pd.DataFrame(iris.data)
X.columns=['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']
y=pd.DataFrame(iris.target)
y.columns=['Targets']
model=KMeans(n_clusters=3)
model.fit(X)
plt.figure(figsize=(14,14))
colormap=np.array(['red','lime','black'])
plt.subplot(2,2,1)
plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y.Targets],s=40)
plt.title('Real clusters')
plt.xlabel('petal Length')
plt.ylabel('peta; Width')
plt.subplot(2,2,2)
plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[model.labels_],s=40)
plt.title('KMeans clusters')
plt.xlabel('petal Length')
plt.ylabel('peta; Width')

from sklearn import preprocessing
scaler=preprocessing.StandardScaler()
scaler.fit(X)
xsa=scaler.transform(X)
xs=pd.DataFrame(xsa,columns=X.columns)

from sklearn.mixture import GaussianMixture
gmm=GaussianMixture(n_components=3)
gmm.fit(xs)
gmm_y=gmm.predict(xs)
plt.subplot(2,2,3)
plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[gmm_y],s=40)
plt.title('GMM clustering')
plt.xlabel('petal Length')
plt.ylabel('peta; Width')
print('Observation: ')

output:
3 graphs
---------------------------------------------------------------------------------------------------
#9th program
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import datasets
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
# Load dataset
iris=datasets.load_iris()
print("Iris Data set loaded...")
# Split the data into train and test samples
x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target,test_size=0.1)
print("Dataset is split into training and testing...")
print("Size of trainng data and its label",x_train.shape,y_train.shape)
print("Size of testing data and its label",x_test.shape, y_test.shape)
# Prints Label no. and their names
for i in range(len(iris.target_names)):
    print("Label", i , "-",str(iris.target_names[i]))
# Create object of KNN classifier
classifier = KNeighborsClassifier(n_neighbors=1)
# Perform Training
classifier.fit(x_train, y_train)
# Perform testing
y_pred=classifier.predict(x_test)
# Display the results
print("Results of Classification using KNN with K=1 ")
for r in range(0,len(x_test)):
    print(" Sample:", str(x_test[r]), " Actual-label:", str(y_test[r]), " Predicted-label:",str(y_pred[r]))
print("Classification Accuracy :" , classifier.score(x_test,y_test));
cm=confusion_matrix(y_test,y_pred)
print('confusion matrix\n',cm)
print('correct prediction\n',accuracy_score(y_test,y_pred))
print('wrong prediction\n',(1-accuracy_score(y_test,y_pred)))

output:
Iris Data set loaded...
Dataset is split into training and testing...
Size of trainng data and its label (135, 4) (135,)
Size of testing data and its label (15, 4) (15,)
Label 0 - setosa
Label 1 - versicolor
Label 2 - virginica
Results of Classification using KNN with K=1 
 Sample: [4.6 3.6 1.  0.2]  Actual-label: 0  Predicted-label: 0
 Sample: [5.7 3.  4.2 1.2]  Actual-label: 1  Predicted-label: 1
 Sample: [5.2 4.1 1.5 0.1]  Actual-label: 0  Predicted-label: 0
 Sample: [4.8 3.4 1.9 0.2]  Actual-label: 0  Predicted-label: 0
 Sample: [7.4 2.8 6.1 1.9]  Actual-label: 2  Predicted-label: 2
 Sample: [5.7 2.8 4.5 1.3]  Actual-label: 1  Predicted-label: 1
 Sample: [4.9 2.4 3.3 1. ]  Actual-label: 1  Predicted-label: 1
 Sample: [7.6 3.  6.6 2.1]  Actual-label: 2  Predicted-label: 2
 Sample: [5.2 3.5 1.5 0.2]  Actual-label: 0  Predicted-label: 0
 Sample: [6.5 2.8 4.6 1.5]  Actual-label: 1  Predicted-label: 1
 Sample: [6.9 3.1 4.9 1.5]  Actual-label: 1  Predicted-label: 1
 Sample: [5.1 2.5 3.  1.1]  Actual-label: 1  Predicted-label: 1
 Sample: [6.5 3.  5.8 2.2]  Actual-label: 2  Predicted-label: 2
 Sample: [5.6 3.  4.5 1.5]  Actual-label: 1  Predicted-label: 1
 Sample: [5.9 3.  5.1 1.8]  Actual-label: 2  Predicted-label: 2
Classification Accuracy : 1.0
confusion matrix
 [[4 0 0]
 [0 7 0]
 [0 0 4]]
correct prediction
 1.0
wrong prediction
 0.0
------------------------------------------------------------------------------------------------
#10th program
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
def kernel(point,xmat,k):
    m,n=np.shape(xmat)
    weight=np.mat(np.eye(m))
    for j in range(m):
        diff=point-X[j]
        weight[j,j]=np.exp(diff*diff.T/(-2.0*k**2))
    return weight
def localweight(point,xmat,ymat,k):
    wei=kernel(point,xmat,k)
    w=(X.T*(wei*X)).I*(X.T*(wei*ymat.T))
    return w
def localweightregression(xmat,ymat,k):
    m,n=np.shape(xmat)
    ypred=np.zeros(m)
    for i in range(m):
        ypred[i]=xmat[i]*localweight(xmat[i],xmat,ymat,k)
    return ypred
def graphplot(X,ypred):
    sortindex=X[:,1].argsort(0)
    xsort=X[sortindex][:,0]
    fig=plt.figure()
    ax=fig.add_subplot(1,1,1)
    ax.scatter(bill,tip,color='green')
    ax.plot(xsort[:,1],ypred[sortindex],color='red',linewidth=5)
    plt.xlabel('Bill')
    plt.ylabel('tip')
    plt.show()

                           
data=pd.read_csv('data10_tips.csv')
bill=np.array(data.total_bill)
tip=np.array(data.tip)
mbill=np.mat(bill)
mtip=np.mat(tip)
m=np.shape(mbill)[1]
one=np.mat(np.ones(m))
X=np.hstack((one.T,mbill.T))
ypred=localweightregression(X,mtip,0.5)
graphplot(X,ypred)

output: graph




            
            
            
        
            

            


    
    